{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6edb1dbe-20ad-49d3-aabf-27a0438da7e0",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b1d6d12-e770-47ce-a9eb-fb93b7a2b755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "import tqdm.notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Union, Tuple\n",
    "import ale_py\n",
    "import numpy as np \n",
    "from PIL import Image\n",
    "import gymnasium.utils.save_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "011e03a4-a0ea-4ce2-9f00-e09b9b4c2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "              tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d26ffb73-c4fa-4a16-b477-358576552948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(prefix: Optional[str] = None, suffix: Optional[str] = None, separator: str = '_') -> Optional[str]:\n",
    "    return prefix and prefix + separator + suffix or suffix or None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e580a6f-a987-40ec-9864-2e296bdd9ba6",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120abf50-431d-4c8c-bc71-e760aa7c08b4",
   "metadata": {},
   "source": [
    "Create the environment. You can use any ATARI environment from [here](https://gymnasium.farama.org/environments/atari/), but prefer to use environments with discrete action space with fewer actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a642f38e-23b0-4f40-b5ae-9e7d044abea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27006604-30b9-424a-83d5-77fac19eaca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c9241b5-510e-4a18-bcde-8bf3c5ac2b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.2+c9d4b19)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "train_env = gym.make_vec(\"ALE/Boxing-v5\", render_mode='rgb_array', num_envs=batch_size)\n",
    "valid_env = gym.make_vec(\"ALE/Boxing-v5\", render_mode='rgb_array', num_envs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6828a718-db96-42f6-890f-265163fdedb9",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037a88e3-0524-4d4f-9bd5-eb035d80fece",
   "metadata": {},
   "source": [
    "Create a replay buffer to hold game history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a761715-1ef6-4ffd-b710-1758f292888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, max_size: int, observation_space: gym.spaces.Space, action_space: gym.spaces.Space, seed: Optional[int] = None):\n",
    "        \"\"\"Stores the replay history with a maximum of `max_size` entries, removing old entries as needed.\n",
    "\n",
    "        Parameters:\n",
    "            max_size: maximal number of entries to keep\n",
    "            observation_space: specification of the observation space\n",
    "            action_space: specification of the action space\n",
    "            seed: seed to initialize the internal random number generator for reproducibility\"\"\"\n",
    "\n",
    "        self.current_observations = np.zeros((max_size, *observation_space.shape), dtype=observation_space.dtype)\n",
    "        self.next_observations = np.zeros((max_size, *observation_space.shape), dtype=observation_space.dtype)\n",
    "        self.actions = np.zeros((max_size, *action_space.shape), dtype=action_space.dtype)\n",
    "        self.rewards = np.zeros((max_size,), dtype=np.float32)\n",
    "        self.dones = np.zeros((max_size,), dtype=np.float32)\n",
    "        \n",
    "        self.max_size = max_size\n",
    "        self.rng = np.random.default_rng(seed=seed)\n",
    "        self.buffer_pointer = 0\n",
    "        self.current_size = 0\n",
    "        \n",
    "    def add(self, current_observations: np.ndarray, actions: np.ndarray, rewards: np.ndarray, next_observations: np.ndarray, dones: np.ndarray) -> None:\n",
    "        \"\"\"Add a new entry to the buffer.\n",
    "\n",
    "        Parameters:\n",
    "            current_observations: environment state observed at the current step\n",
    "            actions: action taken by the model\n",
    "            rewards: reward received after taking the action\n",
    "            next_observations: environment state obversed after taking the action\n",
    "            dones: whether the episode has ended or not\"\"\"\n",
    "\n",
    "        batch_size = current_observations.shape[0]\n",
    "        idxs = (np.arange(batch_size) + self.buffer_pointer) % self.max_size\n",
    "\n",
    "        self.current_observations[idxs] = current_observations\n",
    "        self.actions[idxs] = actions\n",
    "        self.rewards[idxs] = rewards\n",
    "        self.next_observations[idxs] = next_observations\n",
    "        self.dones[idxs] = dones\n",
    "\n",
    "        self.buffer_pointer = (idxs[-1] + 1) % self.max_size\n",
    "        self.current_size = min(self.max_size, self.current_size + batch_size)\n",
    "    \n",
    "    def sample(self, n_samples: int, replace: bool = True) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Randomly samples `n_samples` from the buffer.\n",
    "\n",
    "        Parameters:\n",
    "            n_samples: number of samples to select\n",
    "            replace: sample with or without replacement\n",
    "\n",
    "        Returns:\n",
    "            current observations, actions, rewards, next observations, dones\"\"\"\n",
    "\n",
    "        return self[self.rng.choice(self.current_size, size=n_samples, replace=replace)]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clears the buffer\"\"\"\n",
    "\n",
    "        self.buffer_pointer = 0\n",
    "        self.current_size = 0\n",
    "\n",
    "    def __getitem__(self, index: Union[int , np.ndarray]) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Gets a sample at `index`\n",
    "\n",
    "        Parameters:\n",
    "            index: index of the sample to get\n",
    "\n",
    "        Returns:\n",
    "            current observations, actions, rewards, next observations, dones\"\"\"\n",
    "\n",
    "        return (\n",
    "            self.current_observations[index],\n",
    "            self.actions[index],\n",
    "            self.rewards[index],\n",
    "            self.next_observations[index],\n",
    "            self.dones[index]\n",
    "        )\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the number of entries in the buffer\"\"\"\n",
    "\n",
    "        return self.current_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669485c5-5787-4ffa-82f2-58f6f0acc151",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc62c869-195b-4323-bdb4-dd53879455c9",
   "metadata": {},
   "source": [
    "Implement your model. Most if not all ATARI environments have an image observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a572a086-e9ce-4194-8809-54a8e05477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_model(\n",
    "    observation_space: gym.spaces.Space,\n",
    "    action_space: gym.spaces.Space,\n",
    "    features: int,\n",
    "    blocks: int, \n",
    "    activation: str | keras.layers.Activation | None = 'relu',\n",
    "    dropout_rate: float = 0.,\n",
    "    multiply_freq: int = 1,\n",
    "    name: str | None = None\n",
    " ) -> keras.Model:\n",
    "    inputs = keras.layers.Input(observation_space.shape, name=get_name(name, \"Input\"), dtype=observation_space.dtype)\n",
    "\n",
    "    x = keras.layers.Rescaling(1.0 / 255, name=get_name(name, \"Rescaling\"))(inputs)\n",
    "\n",
    "    for idx in range(blocks):\n",
    "        features = (idx + 1) * features\n",
    "\n",
    "        x = keras.layers.Conv2D(features, 3, padding=\"same\", activation=activation, name=get_name(name, f\"Conv2D_{idx}\"))(x)\n",
    "        x = keras.layers.MaxPooling2D((2, 2), name=get_name(name, f\"MaxPooling2D_{idx}\"))(x)\n",
    "        \n",
    "    x = keras.layers.BatchNormalization(name=get_name(name, f\"BatchNormalization_{idx}\"))(x)\n",
    "    x = keras.layers.GlobalAveragePooling2D(name=get_name(name, \"GlobalAveragePooling2D\"))(x)\n",
    "        \n",
    "    outputs = keras.layers.Dense(action_space.n, name=get_name(name, \"Output\"))(x)\n",
    "    \n",
    "    return keras.Model(inputs, outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8f6a53-ffa2-44ee-985b-a8bd921592fb",
   "metadata": {},
   "source": [
    "# Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e44c932-378b-4bff-b11d-20b4b879de9a",
   "metadata": {},
   "source": [
    "Implement the sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2de070f1-c6b7-4a4a-81ae-e471f159e6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "    \n",
    "    def __init__(self, epsilon: float, seed: int | None = None, greedy: bool = False):\n",
    "        \"\"\"Selects a random action with probability `epsilon` otherwise if `greedy` selects the most probable action given by the model,\n",
    "        if not `greedy` samples the action from the distribution given by the model.\n",
    "\n",
    "        Parameters:\n",
    "            epsilon: the probability to select a random action\n",
    "            seed: seed to initialize the internal random number generator for reproducibility\n",
    "            greedy: select the most probable action\"\"\"\n",
    "        self.epsilon = epsilon\n",
    "        self.greedy = greedy\n",
    "        self.rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "    def _sample(self, probabilities: np.ndarray) -> np.ndarray:\n",
    "        cumsum = np.cumsum(probabilities, axis=-1)\n",
    "        probs = self.rng.uniform(size=(cumsum.shape[0], 1))\n",
    "        msk = cumsum > probs\n",
    "        return np.argmax(msk, axis=-1)\n",
    "\n",
    "    def __call__(self, probabilities: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Select an action given the `probabilities`\n",
    "\n",
    "        Parameters:\n",
    "            probabilities: batch of probabilities for each action\n",
    "\n",
    "        Returns:\n",
    "            batch of indices of the selected actions\"\"\"\n",
    "        batch, *_ = probabilities.shape\n",
    "        probs = self.rng.uniform(size=batch)\n",
    "\n",
    "        return np.where(\n",
    "            probs < self.epsilon,\n",
    "            self.rng.choice(probabilities.shape[1], size=batch),\n",
    "            np.argmax(probabilities, axis=-1) if self.greedy else self._sample(probabilities),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2cc1a3-3e45-4aac-8936-d5d45cc256f1",
   "metadata": {},
   "source": [
    "# Play the game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aa11b5-0ca1-4bfb-91a0-33a4f8922ae9",
   "metadata": {},
   "source": [
    "Implement interacting with the environment and storing entries to the replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b333e6b3-853a-4cb7-aea3-8c501d0247d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(\n",
    "    model: keras.Model,\n",
    "    buffer: ReplayBuffer,\n",
    "    env: gym.Env,\n",
    "    steps: int,\n",
    "    sampler: Sampler,\n",
    "    observations: np.ndarray | None = None,\n",
    "    one_episode: bool = False\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Plays the environment `env` using model `model` for a total of `steps` steps.\n",
    "\n",
    "    Arguments:\n",
    "        model: model to use\n",
    "        buffer: buffer to store results to\n",
    "        env: environment to use\n",
    "        steps: total number of steps to record\n",
    "        sampler: sampler to use\n",
    "        observations: observation to start from\n",
    "        one_episode: exist as soon as one of the environments finishes\n",
    "\n",
    "    Returns:\n",
    "        the last observations\n",
    "    \"\"\"\n",
    "    if observations is None:\n",
    "        observations, _ = env.reset()\n",
    "\n",
    "    for _ in range(steps // env.num_envs):\n",
    "        res = model(observations, training=False).numpy()\n",
    "        actions = sampler(res)\n",
    "        new_observations, rewards, terminateds, truncated, _ = env.step(actions) # new_observations, rewards, terminateds, truncated, info\n",
    "\n",
    "        dones = terminateds | truncated\n",
    "\n",
    "        buffer.add(\n",
    "            current_observations=observations,\n",
    "            actions=actions,\n",
    "            rewards=rewards,\n",
    "            next_observations=new_observations,\n",
    "            dones=dones,\n",
    "        )\n",
    "        \n",
    "        if one_episode and np.any(dones):\n",
    "            observations = None\n",
    "            break\n",
    "        \n",
    "        observations = new_observations\n",
    "    \n",
    "    return observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971ad3a7-0487-49d3-9ca4-6b6bbc3fa450",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcbeef8-70fb-44ae-92e9-0baea77e8f0e",
   "metadata": {},
   "source": [
    "Implement double q learning loss. Don't forget to stop the gradient for q_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0108671b-cd2a-4254-b3ec-0f39ebb02b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QDataset(keras.utils.PyDataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        steps_per_epoch: int,\n",
    "        batch_size: int,\n",
    "        buffer: ReplayBuffer,\n",
    "        action_space: gym.spaces.Space,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.buffer = buffer\n",
    "        self.batch_size = batch_size\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "        self._eye = np.eye(action_space.n)\n",
    "        self._answer = np.zeros(batch_size, dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.steps_per_epoch\n",
    "\n",
    "    def __getitem__(self, idx: int) -> np.ndarray:\n",
    "        a, b, c, d, e = self.buffer.sample(self.batch_size)\n",
    "        b = self._eye[b]\n",
    "\n",
    "        return (a, b, c, d, e), self._answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9775cbc-77a8-4b02-ab0a-3fbb22b3e958",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLoss(keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, gamma: float, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        q_current: keras.KerasTensor,\n",
    "        q_next: keras.KerasTensor,\n",
    "        rewards: keras.KerasTensor,\n",
    "        actions: keras.KerasTensor,\n",
    "        dones: keras.KerasTensor\n",
    "    ) -> keras.KerasTensor:\n",
    "        q_ref = rewards + self.gamma * (1 - dones) * keras.ops.max(q_next, axis=-1)\n",
    "\n",
    "        return keras.ops.square(keras.ops.sum(q_current * actions, axis=-1) - q_ref)\n",
    "\n",
    "    def get_config(self) -> dict:\n",
    "        config = super().get_config()\n",
    "        config['gamma'] = self.gamma\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "945d322c-9e9a-480a-9643-fddc0ce3acd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_model(\n",
    "    observation_space: gym.spaces.Space,\n",
    "    action_space: gym.spaces.Space,    \n",
    "    features: int,\n",
    "    blocks: int, \n",
    "    activation: str | keras.layers.Activation | None = 'relu',\n",
    "    dropout_rate: float = 0.,\n",
    "    multiply_freq: int = 1,\n",
    "    name: str | None = None,\n",
    "    gamma: float = 0.99\n",
    ") -> keras.Model:\n",
    "    \"\"\"Creates a combined model for Q-learning training\n",
    "\n",
    "    Arguments:\n",
    "        input_features: model input vector size\n",
    "        features: initial model embedding size\n",
    "        out_features: model output vector size\n",
    "        block: number of perceptron layers\n",
    "        activation: intermediate model activation\n",
    "        dropout_rate: nuff said\n",
    "        multifly_freq: doubles embedding size every `multiply_freq` blocks\n",
    "        name: model name,\n",
    "        gamma: rewards discount\n",
    "    Returns:\n",
    "        A q-model, a target q-model, a combined model\n",
    "    \"\"\"\n",
    "    model = get_model(\n",
    "        observation_space=observation_space,\n",
    "        action_space=action_space,\n",
    "        features=features,\n",
    "        blocks=blocks,\n",
    "        activation=activation,\n",
    "        dropout_rate=dropout_rate,\n",
    "        multiply_freq=multiply_freq,\n",
    "        name=get_name(name, \"model\"),\n",
    "    )\n",
    "    \n",
    "    target_model = get_model(\n",
    "        observation_space=observation_space,\n",
    "        action_space=action_space,\n",
    "        features=features,\n",
    "        blocks=blocks,\n",
    "        activation=activation,\n",
    "        multiply_freq=multiply_freq,\n",
    "        name=get_name(name, \"target_model\"),\n",
    "    )\n",
    "\n",
    "    target_model.trainable = False\n",
    "    target_model.set_weights(model.get_weights())\n",
    "\n",
    "    current_observation = keras.layers.Input(observation_space.shape, dtype=observation_space.dtype, name=get_name(name, \"curr_observ\"))\n",
    "    next_observation = keras.layers.Input(observation_space.shape, dtype=observation_space.dtype, name=get_name(name, \"next_observ\"))\n",
    "    current_action = keras.layers.Input((action_space.n,), dtype=\"float32\", name=get_name(name, \"curr_action\"))\n",
    "    rewards = keras.layers.Input((), dtype=\"float32\", name=get_name(name, \"rewards\"))\n",
    "    dones = keras.layers.Input((), dtype=\"float32\", name=get_name(name, \"dones\"))\n",
    "\n",
    "    model_res = model(current_observation)\n",
    "    target_model_res = target_model(next_observation)\n",
    "\n",
    "    loss = QLoss(gamma=gamma, name=get_name(name, \"Q_loss\"))(model_res, target_model_res, rewards, current_action, dones)\n",
    "\n",
    "    return (\n",
    "        model, \n",
    "        target_model, \n",
    "        keras.Model(\n",
    "            inputs=[\n",
    "                current_observation,\n",
    "                current_action,\n",
    "                rewards,\n",
    "                next_observation,\n",
    "                dones,\n",
    "            ],\n",
    "            outputs=loss,\n",
    "            name=name\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d1ca365-7e45-419b-92d4-89f3947c6644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(\n",
    "    model: keras.Model,\n",
    "    buffer: ReplayBuffer,\n",
    "    env: gym.Env,\n",
    "    steps: int,\n",
    "    sampler: Sampler,\n",
    "    observations: np.ndarray | None = None,\n",
    "    one_episode: bool = False\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Plays the environment `env` using model `model` for a total of `steps` steps.\n",
    "\n",
    "    Arguments:\n",
    "        model: model to use\n",
    "        buffer: buffer to store results to\n",
    "        env: environment to use\n",
    "        steps: total number of steps to record\n",
    "        sampler: sampler to use\n",
    "        observations: observation to start from\n",
    "        one_episode: exist as soon as one of the environments finishes\n",
    "\n",
    "    Returns:\n",
    "        the last observations\n",
    "    \"\"\"\n",
    "    if observations is None:\n",
    "        observations, _ = env.reset()\n",
    "\n",
    "    for _ in range(steps // env.num_envs):\n",
    "        res = model(observations, training=False).numpy()\n",
    "        actions = sampler(res)\n",
    "        new_observations, rewards, terminateds, truncated, _ = env.step(actions) # new_observations, rewards, terminateds, truncated, info\n",
    "\n",
    "        dones = terminateds | truncated\n",
    "\n",
    "        buffer.add(\n",
    "            current_observations=observations,\n",
    "            actions=actions,\n",
    "            rewards=rewards,\n",
    "            next_observations=new_observations,\n",
    "            dones=dones,\n",
    "        )\n",
    "        \n",
    "        if one_episode and np.any(dones):\n",
    "            observations = None\n",
    "            break\n",
    "        \n",
    "        observations = new_observations\n",
    "    \n",
    "    return observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbc437f8-a897-494f-84fb-8ffc0d6621cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelUpdateCallback(keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, frequency: int, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.frequency = frequency\n",
    "        \n",
    "    def on_train_batch_end(self, batch: int, logs=None):\n",
    "        if (batch + 1) % self.frequency == 0:\n",
    "            self.model.get_layer(\n",
    "                f'{self.model.name}_target_model'\n",
    "            ).set_weights(\n",
    "                self.model.get_layer(\n",
    "                    f'{self.model.name}_model'\n",
    "                ).get_weights()\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a25f589b-a57e-4f7c-8764-6b736b9ace45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvalCallback(keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, env: gym.Env, max_steps: int, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.env = env\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "        self.buffer = ReplayBuffer(\n",
    "            max_size=max_steps,\n",
    "            observation_space=env.single_observation_space,\n",
    "            action_space=env.single_observation_space,\n",
    "        )\n",
    "\n",
    "        self.sampler = Sampler(epsilon=0., greedy=True)\n",
    "\n",
    "    def on_epoch_end(self, epoch: int, logs=None):\n",
    "        if logs is None:\n",
    "            return\n",
    "\n",
    "        model = self.model.get_layer(\n",
    "            f'{self.model.name}_model'\n",
    "        )\n",
    "\n",
    "        self.buffer.clear()\n",
    "\n",
    "        play_game(\n",
    "            model=model,\n",
    "            buffer=self.buffer,\n",
    "            env=self.env,\n",
    "            steps=self.max_steps,\n",
    "            sampler=self.sampler,\n",
    "            one_episode=True,\n",
    "        )\n",
    "\n",
    "        rewards = self.buffer.rewards[:len(self.buffer)].reshape((-1, self.env.num_envs))\n",
    "        logs['score'] = np.mean(np.sum(rewards, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a079053d-5ff4-44d7-a459-c5bc40f8cd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SometimesPlayCallback(keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        buffer: ReplayBuffer,\n",
    "        sampler: Sampler,\n",
    "        frequency: int,\n",
    "        steps_per_play: int,\n",
    "        epsilon_decay: keras.optimizers.schedules.PolynomialDecay | None = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.env = env\n",
    "        self.buffer = buffer\n",
    "        self.sampler = sampler\n",
    "        self.frequency = frequency\n",
    "        self.steps_per_play = steps_per_play\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        self.observations = None\n",
    "\n",
    "    def on_epoch_begin(self, epoch: int, logs=None):\n",
    "        if self.epsilon_decay is not None:\n",
    "            self.sampler.epsilon = self.epsilon_decay(epoch)\n",
    "        if epoch == 0:\n",
    "            model = self.model.get_layer(\n",
    "                f'{self.model.name}_model'\n",
    "            )\n",
    "\n",
    "            self.buffer.clear()\n",
    "            \n",
    "            self.observations = play_game(\n",
    "                model=model,\n",
    "                buffer=self.buffer,\n",
    "                env=self.env,\n",
    "                steps=self.steps_per_play,\n",
    "                sampler=self.sampler,\n",
    "                observations=self.observations\n",
    "            )\n",
    "\n",
    "    def on_train_batch_begin(self, batch_num: int, logs=None):\n",
    "        if (batch_num + 1) % self.frequency:\n",
    "            return\n",
    "        \n",
    "        model = self.model.get_layer(\n",
    "            f'{self.model.name}_model'\n",
    "        )\n",
    "\n",
    "        self.observations = play_game(\n",
    "            model=model,\n",
    "            buffer=self.buffer,\n",
    "            env=self.env,\n",
    "            steps=self.steps_per_play,\n",
    "            sampler=self.sampler,\n",
    "            observations=self.observations\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78458a1f-054a-463a-934f-ac3f669c0e27",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb0aaa7-6ae5-45dc-9c52-04c474ec3ecc",
   "metadata": {},
   "source": [
    "Create models, replay buffers, sampler, optimizer, epsilon decay etc. Implement training loop, show training progress and perform model evaluation once in a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "139138b8-f63c-4a2f-8793-4f96d25a353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, target_model, combined_model = get_combined_model(\n",
    "    observation_space=train_env.single_observation_space,\n",
    "    action_space=train_env.single_action_space,\n",
    "    features=16,\n",
    "    blocks=4,\n",
    "    activation='relu',\n",
    "    dropout_rate=0.2,\n",
    "    multiply_freq=1,\n",
    "    name='boxing',\n",
    "    gamma=0.99\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f8feb06-9419-4516-9d00-3e4a7fd206dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 2 ** 15     # 32_768\n",
    "steps_per_play = 2 ** 11  # 2048\n",
    "steps_per_epoch = 2 ** 10 # 1024\n",
    "\n",
    "end_epsilon = 0.1\n",
    "epochs = 128\n",
    "decay_epochs = epochs // 2\n",
    "update_frequency = 512\n",
    "play_frequency = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c0533e5-62f6-47b7-bc05-21249aba51a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = Sampler(epsilon=1, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c0583c08-0fba-4c11-b52e-e48ac20af829",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_decay = keras.optimizers.schedules.PolynomialDecay(1., decay_epochs, end_learning_rate=end_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6be17d6c-ce43-4fa9-9135-9fdb0d814f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_buffer = ReplayBuffer(\n",
    "    max_size=buffer_size,\n",
    "    observation_space=train_env.single_observation_space,\n",
    "    action_space=train_env.single_action_space\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cbe0cef1-99a7-484f-9987-ddd1281d578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = QDataset(\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    batch_size=train_env.num_envs,\n",
    "    buffer=train_buffer,\n",
    "    action_space=train_env.single_action_space\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "81abe5db-2602-4a8f-b326-a513e34187a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'foo{epoch}'.format(epoch=42)\n",
    "\n",
    "checkpoint_filepath = './history/checkpoint.model-{epoch}.keras'\n",
    "\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='loss',\n",
    "    mode='min',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31aead45-7c26-48d9-8be6-81775bc070de",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ModelUpdateCallback(update_frequency),\n",
    "    ModelEvalCallback(valid_env, 1000),\n",
    "    SometimesPlayCallback(\n",
    "        env=train_env,\n",
    "        buffer=train_buffer,\n",
    "        sampler=sampler,\n",
    "        frequency=play_frequency,\n",
    "        steps_per_play=steps_per_play,\n",
    "        epsilon_decay=epsilon_decay,\n",
    "    ),\n",
    "    model_checkpoint_callback\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53ed1420-2702-4fc0-a33d-e5c1ef4f68eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model.compile(\n",
    "    loss=(lambda y_true, y_pred: keras.ops.mean(y_pred)),\n",
    "    optimizer=keras.optimizers.Adam(1e-3, clipnorm=5, weight_decay=2e-5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "509c3c33-422b-447e-ac42-fc9496e2adcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = play_game(\n",
    "    model=model,\n",
    "    buffer=train_buffer,\n",
    "    env=train_env,\n",
    "    steps=steps_per_play,\n",
    "    sampler=sampler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e3f5e0e9-0097-4a85-b44e-5eeb00f44bc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 77ms/step - loss: 0.1745 - score: -31.0000\n",
      "Epoch 2/128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/timohavdk/miniconda3/envs/p11/lib/python3.11/site-packages/keras/src/saving/serialization_lib.py:390: UserWarning: The object being serialized includes a `lambda`. This is unsafe. In order to reload the object, you will have to pass `safe_mode=False` to the loading function. Please avoid using `lambda` in the future, and use named Python functions instead. This is the `lambda` being serialized:     loss=(lambda y_true, y_pred: keras.ops.mean(y_pred)),\n",
      "\n",
      "  return {key: serialize_keras_object(value) for key, value in obj.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 1.6662 - score: -39.0000\n",
      "Epoch 3/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 0.2442 - score: -20.0000\n",
      "Epoch 4/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 78ms/step - loss: 4.6593 - score: -39.0000\n",
      "Epoch 5/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 80ms/step - loss: 0.2404 - score: -63.0000\n",
      "Epoch 6/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 82ms/step - loss: 3.1869 - score: -20.0000\n",
      "Epoch 7/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 0.1628 - score: 2.0000\n",
      "Epoch 8/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 78ms/step - loss: 1.6639 - score: -66.0000\n",
      "Epoch 9/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 0.2726 - score: -39.0000\n",
      "Epoch 10/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 79ms/step - loss: 0.1707 - score: -31.0000\n",
      "Epoch 11/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 82ms/step - loss: 0.3239 - score: -31.0000\n",
      "Epoch 12/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 79ms/step - loss: 3.5961 - score: -31.0000\n",
      "Epoch 13/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 78ms/step - loss: 0.7917 - score: -18.0000\n",
      "Epoch 14/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 78ms/step - loss: 0.5232 - score: -33.0000\n",
      "Epoch 15/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 76ms/step - loss: 8.5539 - score: -66.0000\n",
      "Epoch 16/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 76ms/step - loss: 47.6318 - score: -19.0000\n",
      "Epoch 17/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 3.1758 - score: -18.0000\n",
      "Epoch 18/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 77ms/step - loss: 1.0268 - score: -15.0000\n",
      "Epoch 19/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 76ms/step - loss: 0.0816 - score: -21.0000\n",
      "Epoch 20/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 76ms/step - loss: 0.0707 - score: -32.0000\n",
      "Epoch 21/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 78ms/step - loss: 0.0722 - score: -25.0000\n",
      "Epoch 22/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 78ms/step - loss: 0.0890 - score: -64.0000\n",
      "Epoch 23/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 77ms/step - loss: 0.1661 - score: -17.0000\n",
      "Epoch 24/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 76ms/step - loss: 0.0913 - score: -22.0000\n",
      "Epoch 25/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 79ms/step - loss: 0.0965 - score: -29.0000\n",
      "Epoch 26/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 78ms/step - loss: 0.1167 - score: -4.0000\n",
      "Epoch 27/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 77ms/step - loss: 0.0847 - score: -24.0000\n",
      "Epoch 28/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 78ms/step - loss: 0.1456 - score: -31.0000\n",
      "Epoch 29/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 1.7635 - score: -66.0000\n",
      "Epoch 30/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 79ms/step - loss: 0.8407 - score: -6.0000\n",
      "Epoch 31/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 0.8198 - score: -21.0000\n",
      "Epoch 32/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 77ms/step - loss: 0.8563 - score: -31.0000\n",
      "Epoch 33/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 78ms/step - loss: 0.0945 - score: -26.0000\n",
      "Epoch 34/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 79ms/step - loss: 0.0618 - score: -21.0000\n",
      "Epoch 35/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 0.0532 - score: -9.0000\n",
      "Epoch 36/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 81ms/step - loss: 0.0566 - score: -25.0000\n",
      "Epoch 37/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 78ms/step - loss: 0.0626 - score: -34.0000\n",
      "Epoch 38/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 78ms/step - loss: 0.0613 - score: -41.0000\n",
      "Epoch 39/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 0.0630 - score: -28.0000\n",
      "Epoch 40/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 0.0544 - score: -16.0000\n",
      "Epoch 41/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 79ms/step - loss: 0.0519 - score: -4.0000\n",
      "Epoch 42/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 78ms/step - loss: 0.0941 - score: -39.0000\n",
      "Epoch 43/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 0.6237 - score: -21.0000\n",
      "Epoch 44/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 78ms/step - loss: 0.9970 - score: -10.0000\n",
      "Epoch 45/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 78ms/step - loss: 0.9703 - score: -10.0000\n",
      "Epoch 46/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 77ms/step - loss: 0.8864 - score: -26.0000\n",
      "Epoch 47/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 77ms/step - loss: 0.1183 - score: -8.0000\n",
      "Epoch 48/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 0.0662 - score: 1.0000\n",
      "Epoch 49/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 76ms/step - loss: 0.0596 - score: -1.0000\n",
      "Epoch 50/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 79ms/step - loss: 0.0868 - score: -12.0000\n",
      "Epoch 51/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 0.0936 - score: -9.0000\n",
      "Epoch 52/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 0.0778 - score: 5.0000\n",
      "Epoch 53/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 0.0904 - score: -12.0000\n",
      "Epoch 54/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 0.0640 - score: -10.0000\n",
      "Epoch 55/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 78ms/step - loss: 0.0597 - score: 3.0000\n",
      "Epoch 56/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 78ms/step - loss: 0.1190 - score: -42.0000\n",
      "Epoch 57/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 78ms/step - loss: 1.2758 - score: -21.0000\n",
      "Epoch 58/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 78ms/step - loss: 1.3248 - score: -72.0000\n",
      "Epoch 59/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 78ms/step - loss: 1.6796 - score: -1.0000\n",
      "Epoch 60/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 1.5557 - score: -8.0000\n",
      "Epoch 61/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 78ms/step - loss: 0.0843 - score: 5.0000\n",
      "Epoch 62/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 76ms/step - loss: 0.0666 - score: -2.0000\n",
      "Epoch 63/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 76ms/step - loss: 0.0577 - score: -25.0000\n",
      "Epoch 64/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 80ms/step - loss: 0.0582 - score: -22.0000\n",
      "Epoch 65/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 78ms/step - loss: 0.0535 - score: -3.0000\n",
      "Epoch 66/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 80ms/step - loss: 0.0599 - score: -16.0000\n",
      "Epoch 67/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 78ms/step - loss: 0.0549 - score: -12.0000\n",
      "Epoch 68/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 80ms/step - loss: 0.0413 - score: -46.0000\n",
      "Epoch 69/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 79ms/step - loss: 0.0420 - score: -13.0000\n",
      "Epoch 70/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 83ms/step - loss: 0.1207 - score: -25.0000\n",
      "Epoch 71/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 76ms/step - loss: 1.8187 - score: -23.0000\n",
      "Epoch 72/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 2.2903 - score: -7.0000\n",
      "Epoch 73/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 78ms/step - loss: 2.9198 - score: -13.0000\n",
      "Epoch 74/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 2.6751 - score: -16.0000\n",
      "Epoch 75/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 76ms/step - loss: 0.1994 - score: -21.0000\n",
      "Epoch 76/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 76ms/step - loss: 0.1581 - score: -4.0000\n",
      "Epoch 77/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 76ms/step - loss: 0.0958 - score: -25.0000\n",
      "Epoch 78/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 0.1119 - score: -78.0000\n",
      "Epoch 79/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 0.0865 - score: -26.0000\n",
      "Epoch 80/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 0.0739 - score: -16.0000\n",
      "Epoch 81/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 78ms/step - loss: 0.0743 - score: 0.0000e+00\n",
      "Epoch 82/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 0.0532 - score: -4.0000\n",
      "Epoch 83/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 77ms/step - loss: 0.0459 - score: -17.0000\n",
      "Epoch 84/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 0.1813 - score: -27.0000\n",
      "Epoch 85/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 3.6192 - score: -12.0000\n",
      "Epoch 86/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 2.9571 - score: -43.0000\n",
      "Epoch 87/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 3.1545 - score: -31.0000\n",
      "Epoch 88/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 76ms/step - loss: 3.7351 - score: -10.0000\n",
      "Epoch 89/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 76ms/step - loss: 0.3797 - score: -4.0000\n",
      "Epoch 90/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 0.1938 - score: -4.0000\n",
      "Epoch 91/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 0.1268 - score: -61.0000\n",
      "Epoch 92/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 76ms/step - loss: 0.2470 - score: -24.0000\n",
      "Epoch 93/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 0.2245 - score: -32.0000\n",
      "Epoch 94/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 0.1788 - score: -14.0000\n",
      "Epoch 95/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 78ms/step - loss: 0.1051 - score: -28.0000\n",
      "Epoch 96/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 76ms/step - loss: 0.0923 - score: -24.0000\n",
      "Epoch 97/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 79ms/step - loss: 0.0689 - score: -5.0000\n",
      "Epoch 98/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 78ms/step - loss: 0.8650 - score: 6.0000\n",
      "Epoch 99/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 76ms/step - loss: 4.6342 - score: -61.0000\n",
      "Epoch 100/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 77ms/step - loss: 5.3806 - score: -60.0000\n",
      "Epoch 101/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 76ms/step - loss: 5.7242 - score: -21.0000\n",
      "Epoch 102/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 78ms/step - loss: 5.8314 - score: -11.0000\n",
      "Epoch 103/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 0.1841 - score: 4.0000\n",
      "Epoch 104/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 0.1624 - score: -96.0000\n",
      "Epoch 105/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 0.1366 - score: -33.0000\n",
      "Epoch 106/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 0.2795 - score: -25.0000\n",
      "Epoch 107/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 0.1882 - score: -37.0000\n",
      "Epoch 108/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 0.1888 - score: -22.0000\n",
      "Epoch 109/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 76ms/step - loss: 0.1336 - score: -15.0000\n",
      "Epoch 110/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 76ms/step - loss: 0.0886 - score: 0.0000e+00\n",
      "Epoch 111/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 0.0861 - score: -17.0000\n",
      "Epoch 112/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 1.6177 - score: -29.0000\n",
      "Epoch 113/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 9.4829 - score: -24.0000\n",
      "Epoch 114/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 76ms/step - loss: 16.0543 - score: -24.0000\n",
      "Epoch 115/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 41.3547 - score: -28.0000\n",
      "Epoch 116/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 22.5189 - score: -41.0000\n",
      "Epoch 117/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 78ms/step - loss: 0.7182 - score: -6.0000\n",
      "Epoch 118/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 77ms/step - loss: 0.4502 - score: -21.0000\n",
      "Epoch 119/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 0.6220 - score: -4.0000\n",
      "Epoch 120/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 0.8621 - score: -47.0000\n",
      "Epoch 121/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 0.5284 - score: -33.0000\n",
      "Epoch 122/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 79ms/step - loss: 0.2399 - score: -18.0000\n",
      "Epoch 123/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 78ms/step - loss: 0.1894 - score: -19.0000\n",
      "Epoch 124/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 0.2536 - score: -6.0000\n",
      "Epoch 125/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 0.1177 - score: 0.0000e+00\n",
      "Epoch 126/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 4.0277 - score: -16.0000\n",
      "Epoch 127/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 77ms/step - loss: 26.9152 - score: -69.0000\n",
      "Epoch 128/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 78ms/step - loss: 39.4699 - score: -19.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fdc5d09d690>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_model.fit(dataset, callbacks=callbacks, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4d37e4a8-ebe2-4cc9-8a1b-4849b0beed7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(2.0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_buffer.rewards.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd5b44e-55cd-43bb-89dd-87c28cc10a9c",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965e5731-8d24-4482-82ed-7226fb18fa5c",
   "metadata": {},
   "source": [
    "Test the model on the environment and get a cool video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa333dc7-227c-4c57-8fa9-dfd562871b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744291135.321130   55616 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5660 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050, pci bus id: 0000:09:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('./models/model-v3.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21fd7a3c-3d86-4c9e-acfb-41206206843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_steps = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4b64d39-14d0-44c8-90c3-869f416a44dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_env = gym.make_vec(\"ALE/Boxing-v5\", render_mode='rgb_array_list', num_envs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72ec281c-d005-499a-b416-4d0e43437a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_gameplay(\n",
    "    model: tf.keras.Model,\n",
    "    max_steps: int = 1000,\n",
    "    env: gym.Env | None = None,\n",
    "):\n",
    "    save_buffer = ReplayBuffer(\n",
    "        max_size=200000,\n",
    "        observation_space=train_env.single_observation_space,\n",
    "        action_space=train_env.single_action_space\n",
    "    )\n",
    "    \n",
    "    save_sampler = Sampler(0, greedy=True)\n",
    "    \n",
    "    _ = play_game(\n",
    "        model=model,\n",
    "        env=env,\n",
    "        steps=max_steps,\n",
    "        sampler=save_sampler,\n",
    "        buffer=save_buffer\n",
    "    )\n",
    "    \n",
    "    render = env.render()\n",
    "\n",
    "    # gym.utils.save_video.save_video(\n",
    "    #     frames=render[0],\n",
    "    #     video_folder=\"videos\",\n",
    "    #     fps=env.metadata[\"render_fps\"],\n",
    "    # )\n",
    "\n",
    "    return render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d4199602-db43-4c00-864f-e10d949e336d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = gym.make_vec(\"ALE/Boxing-v5\", render_mode='rgb_array', num_envs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "79cdcd89-27e0-4f36-95c5-1f3e77b55d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = test_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "37faebc7-31ac-4e59-a57b-5409228e65a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model(observation[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0e46db57-91d9-4f5c-ac8b-ff8fefcafbae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 18), dtype=float32, numpy=\n",
       "array([[158.84975, 158.6238 , 155.97168, 159.64922, 157.33707, 164.28032,\n",
       "        156.86633, 156.56297, 158.59987, 157.19196, 158.47374, 157.69778,\n",
       "        160.59332, 156.52937, 159.25946, 158.40334, 159.13138, 158.55917]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16381e8e-c546-4a4e-b26a-763b1a52edc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744291193.204067   55616 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    }
   ],
   "source": [
    "res = save_gameplay(model, max_steps=saving_steps, env=saving_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03bdbb0a-2a0c-40e4-b4e2-da6b4b96d9ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dc2b4c98-e6b9-4c57-aab1-3591bea32a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/model-v3.keras')\n",
    "target_model.save('./models/target_model-v3.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "28708ccd-588f-4a9f-ada8-d8d4d7072103",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = get_model(\n",
    "    observation_space=train_env.single_observation_space,\n",
    "    action_space=train_env.single_action_space,\n",
    "    features=32,\n",
    "    blocks=4,\n",
    "    activation='relu',\n",
    "    dropout_rate=0.2,\n",
    "    multiply_freq=1,\n",
    "    name='enduro',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fd337b08-8ae4-4ff6-afb9-1bddc80d483c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"enduro\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"enduro\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ enduro_Input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">210</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ enduro_Rescaling (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">210</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ enduro_Conv2D_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">210</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ enduro_MaxPooling2D_0           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">105</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                  │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ enduro_Conv2D_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">105</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ enduro_MaxPooling2D_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                  │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ enduro_Conv2D_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">110,784</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ enduro_MaxPooling2D_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                  │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ enduro_Conv2D_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,327,872</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ enduro_MaxPooling2D_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                  │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ enduro_BatchNormalization_3     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,072</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ enduro_GlobalAveragePooling2D   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ enduro_Output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">13,842</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ enduro_Input (\u001b[38;5;33mInputLayer\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m210\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ enduro_Rescaling (\u001b[38;5;33mRescaling\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m210\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ enduro_Conv2D_0 (\u001b[38;5;33mConv2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m210\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ enduro_MaxPooling2D_0           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m105\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)                  │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ enduro_Conv2D_1 (\u001b[38;5;33mConv2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m105\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ enduro_MaxPooling2D_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)                  │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ enduro_Conv2D_2 (\u001b[38;5;33mConv2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m192\u001b[0m)    │       \u001b[38;5;34m110,784\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ enduro_MaxPooling2D_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m192\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)                  │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ enduro_Conv2D_3 (\u001b[38;5;33mConv2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m768\u001b[0m)    │     \u001b[38;5;34m1,327,872\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ enduro_MaxPooling2D_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m768\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)                  │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ enduro_BatchNormalization_3     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m768\u001b[0m)    │         \u001b[38;5;34m3,072\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ enduro_GlobalAveragePooling2D   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ enduro_Output (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m)             │        \u001b[38;5;34m13,842\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,474,962</span> (5.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,474,962\u001b[0m (5.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,473,426</span> (5.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,473,426\u001b[0m (5.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> (6.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,536\u001b[0m (6.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_test.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
