{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2111a2ba-b98f-4af8-8528-2133af3bbe75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 12:44:22.278534: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744512262.293209  141991 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744512262.301517  141991 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744512262.324159  141991 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744512262.324183  141991 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744512262.324185  141991 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744512262.324230  141991 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-13 12:44:22.331252: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "import tqdm.notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium.utils.save_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3d880f7-2488-49e7-8e1d-1aeda0c0b1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "              tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37ec4bdd-5240-468e-845f-d61ad5e46908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of State Space ->  24\n",
      "Size of Action Space ->  4\n",
      "Max Value of Action ->  1.0\n",
      "Min Value of Action ->  -1.0\n"
     ]
    }
   ],
   "source": [
    "num_states = saving_env.single_observation_space.shape[0]\n",
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "num_actions = saving_env.single_action_space.shape[0]\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "upper_bound = saving_env.single_action_space.high[0]\n",
    "lower_bound = saving_env.single_action_space.low[0]\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17d993cc-48d5-4d64-91ad-6e27a0885eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d93bb55-4f02-4df1-ad83-0c695d6990be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, max_size: int, observation_space: gym.spaces.Space, action_space: gym.spaces.Space, seed: int | None = 42):\n",
    "        \"\"\"Stores the replay history with a maximum of `max_size` entries, removing old entries as needed.\n",
    "\n",
    "        Parameters:\n",
    "            max_size: maximal number of entries to keep\n",
    "            observation_space: specification of the observation space\n",
    "            action_space: specification of the action space\n",
    "            seed: seed to initialize the internal random number generator for reproducibility\"\"\"\n",
    "\n",
    "        self.current_observations = np.zeros((max_size, *observation_space.shape), dtype=observation_space.dtype)\n",
    "        self.next_observations = np.zeros((max_size, *observation_space.shape), dtype=observation_space.dtype)\n",
    "        self.actions = np.zeros((max_size, *action_space.shape), dtype=action_space.dtype)\n",
    "        self.rewards = np.zeros((max_size,), dtype=np.float32)\n",
    "        self.dones = np.zeros((max_size,), dtype=np.float32)\n",
    "        \n",
    "        self.max_size = max_size\n",
    "        self.rng = np.random.default_rng(seed=seed)\n",
    "        self.buffer_pointer = 0\n",
    "        self.current_size = 0\n",
    "        \n",
    "    def add(self, current_observations: np.ndarray, actions: np.ndarray, rewards: np.ndarray, next_observations: np.ndarray, dones: np.ndarray) -> None:\n",
    "        \"\"\"Add a new entry to the buffer.\n",
    "\n",
    "        Parameters:\n",
    "            current_observations: environment state observed at the current step\n",
    "            actions: action taken by the model\n",
    "            rewards: reward received after taking the action\n",
    "            next_observations: environment state obversed after taking the action\n",
    "            dones: whether the episode has ended or not\"\"\"\n",
    "\n",
    "        batch_size = current_observations.shape[0]\n",
    "        idxs = (np.arange(batch_size) + self.buffer_pointer) % self.max_size\n",
    "\n",
    "        self.current_observations[idxs] = current_observations\n",
    "        self.actions[idxs] = actions\n",
    "        self.rewards[idxs] = rewards\n",
    "        self.next_observations[idxs] = next_observations\n",
    "        self.dones[idxs] = dones\n",
    "\n",
    "        self.buffer_pointer = (idxs[-1] + 1) % self.max_size\n",
    "        self.current_size = min(self.max_size, self.current_size + batch_size)\n",
    "    \n",
    "    def sample(self, n_samples: int, replace: bool = True) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Randomly samples `n_samples` from the buffer.\n",
    "\n",
    "        Parameters:\n",
    "            n_samples: number of samples to select\n",
    "            replace: sample with or without replacement\n",
    "\n",
    "        Returns:\n",
    "            current observations, actions, rewards, next observations, dones\"\"\"\n",
    "\n",
    "        return self[self.rng.choice(self.current_size, size=n_samples, replace=replace)]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clears the buffer\"\"\"\n",
    "\n",
    "        self.buffer_pointer = 0\n",
    "        self.current_size = 0\n",
    "\n",
    "    def __getitem__(self, index: int | np.ndarray) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Gets a sample at `index`\n",
    "\n",
    "        Parameters:\n",
    "            index: index of the sample to get\n",
    "\n",
    "        Returns:\n",
    "            current observations, actions, rewards, next observations, dones\"\"\"\n",
    "\n",
    "        return (\n",
    "            self.current_observations[index],\n",
    "            self.actions[index],\n",
    "            self.rewards[index],\n",
    "            self.next_observations[index],\n",
    "            self.dones[index]\n",
    "        )\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the number of entries in the buffer\"\"\"\n",
    "\n",
    "        return self.current_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "7efbe848-33fc-49a4-bf42-1c90620b7670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(\n",
    "    model: keras.Model,\n",
    "    buffer: ReplayBuffer,\n",
    "    env: gym.Env,\n",
    "    steps: int,\n",
    "    observations: np.ndarray | None = None,\n",
    "    one_episode: bool = False,\n",
    "    no_noise: bool = False\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Plays the environment `env` using model `model` for a total of `steps` steps.\n",
    "\n",
    "    Arguments:\n",
    "        model: model to use\n",
    "        buffer: buffer to store results to\n",
    "        env: environment to use\n",
    "        steps: total number of steps to record\n",
    "        observations: observation to start from\n",
    "        one_episode: exist as soon as one of the environments finishes\n",
    "\n",
    "    Returns:\n",
    "        the last observations\n",
    "    \"\"\"\n",
    "\n",
    "    if observations is None:\n",
    "        observations, _ = env.reset()\n",
    "\n",
    "    for _ in range(steps // env.num_envs):\n",
    "        actions = model(observations, training=False).numpy()\n",
    "        \n",
    "        new_observations, rewards, terminateds, truncated, _ = env.step(actions)\n",
    "\n",
    "        equal_observations = np.all(np.isclose(observations, new_observations), axis=1)\n",
    "\n",
    "        dones = terminateds | truncated | equal_observations\n",
    "\n",
    "        rewards = np.where(equal_observations, -100, rewards)\n",
    "\n",
    "        rewards = np.where(rewards == -100, -50, rewards)\n",
    "\n",
    "        print(rewards)\n",
    "        \n",
    "        buffer.add(\n",
    "            current_observations=observations,\n",
    "            actions=actions,\n",
    "            rewards=rewards,\n",
    "            next_observations=new_observations,\n",
    "            dones=dones,\n",
    "        )\n",
    "\n",
    "        if one_episode and np.any(dones):\n",
    "            observations = None\n",
    "            break\n",
    "        \n",
    "        observations = new_observations\n",
    "    \n",
    "    return observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "id": "ef05c1cc-fb82-4b06-8fcd-21d1a7f84c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_model = keras.models.load_model('./history/actor-model-78.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "ee7428df-eb2a-430c-ae84-1fe7f9c73baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "99488307-d32b-459e-93f5-cc486cb68548",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 2 ** 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "ddd04360-ff01-42ec-ae03-5613f317e5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_env = gym.make_vec(\"BipedalWalker-v3\", hardcore=False, render_mode=\"rgb_array_list\", num_envs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "7752a07d-d74c-493e-9f89-2805759ee18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_gameplay(\n",
    "    model: tf.keras.Model,\n",
    "    max_steps: int = 1000,\n",
    "    env: gym.Env | None = None,\n",
    "):\n",
    "    save_buffer = ReplayBuffer(\n",
    "        max_size=buffer_size,\n",
    "        observation_space=saving_env.single_observation_space,\n",
    "        action_space=saving_env.single_action_space\n",
    "    )\n",
    "    \n",
    "    _ = play_game(\n",
    "        model=model,\n",
    "        env=env,\n",
    "        steps=max_steps,\n",
    "        buffer=save_buffer,\n",
    "        one_episode=True\n",
    "    )\n",
    "\n",
    "    render = env.render()\n",
    "\n",
    "    gym.utils.save_video.save_video(\n",
    "        frames=render[0],\n",
    "        video_folder=\"videos\",\n",
    "        fps=env.metadata[\"render_fps\"],\n",
    "    )\n",
    "\n",
    "    return render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "d6cc33a1-3dfb-4d71-a4b9-e8bcf585769c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.1689446]\n",
      "[-0.31295511]\n",
      "[-0.35015011]\n",
      "[-0.29425481]\n",
      "[-0.25102684]\n",
      "[-0.22957689]\n",
      "[-0.2116753]\n",
      "[-0.25465897]\n",
      "[-0.28279477]\n",
      "[-0.28274065]\n",
      "[-0.15310231]\n",
      "[-0.229366]\n",
      "[-0.19697209]\n",
      "[-0.16072549]\n",
      "[-0.14549541]\n",
      "[-0.21468282]\n",
      "[-0.11527488]\n",
      "[-0.19352396]\n",
      "[-0.09312549]\n",
      "[-0.1721883]\n",
      "[-0.071459]\n",
      "[-0.15067394]\n",
      "[-0.05040178]\n",
      "[-0.04100777]\n",
      "[-0.10877983]\n",
      "[-0.15129691]\n",
      "[-0.02252436]\n",
      "[-0.08263274]\n",
      "[-0.03441077]\n",
      "[0.07673378]\n",
      "[0.07246058]\n",
      "[0.10863782]\n",
      "[0.10025559]\n",
      "[0.15275899]\n",
      "[0.21078253]\n",
      "[0.14180039]\n",
      "[0.08141942]\n",
      "[0.1450213]\n",
      "[0.1042174]\n",
      "[0.04458494]\n",
      "[0.06687116]\n",
      "[-0.00880321]\n",
      "[-0.00289559]\n",
      "[0.04605253]\n",
      "[0.10211919]\n",
      "[0.16023058]\n",
      "[0.12038131]\n",
      "[0.06151672]\n",
      "[0.00344715]\n",
      "[-0.05462335]\n",
      "[-0.11385649]\n",
      "[-0.1721641]\n",
      "[-0.23256207]\n",
      "[-0.13813214]\n",
      "[-0.07586938]\n",
      "[0.00659382]\n",
      "[0.08984207]\n",
      "[0.04957263]\n",
      "[-0.11482409]\n",
      "[-0.20378612]\n",
      "[-0.17127663]\n",
      "[-0.22279167]\n",
      "[-0.2866472]\n",
      "[-0.31953219]\n",
      "[-0.2122843]\n",
      "[-0.43288282]\n",
      "[-0.41121063]\n",
      "[-0.41753766]\n",
      "[-0.3705633]\n",
      "[-0.26981747]\n",
      "[-0.35662013]\n",
      "[-0.4035725]\n",
      "[-0.39259273]\n",
      "[-0.40233037]\n",
      "[-0.39809957]\n",
      "[-0.40487868]\n",
      "[-0.38035053]\n",
      "[-0.34950396]\n",
      "[-0.33521926]\n",
      "[-0.37333435]\n",
      "[-0.36898965]\n",
      "[-0.46518183]\n",
      "[-0.52597713]\n",
      "[-0.54633272]\n",
      "[-0.51735103]\n",
      "[-50.]\n"
     ]
    }
   ],
   "source": [
    "res = save_gameplay(actor_model, max_steps=saving_steps, env=saving_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ea7236-7802-4c5e-af2f-94a566686d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
