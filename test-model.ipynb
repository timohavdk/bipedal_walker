{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2111a2ba-b98f-4af8-8528-2133af3bbe75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 23:50:02.840705: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744379402.855674   93994 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744379402.860349   93994 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744379402.872607   93994 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744379402.872628   93994 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744379402.872630   93994 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744379402.872631   93994 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-11 23:50:02.876482: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "import tqdm.notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium.utils.save_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3d880f7-2488-49e7-8e1d-1aeda0c0b1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "              tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37ec4bdd-5240-468e-845f-d61ad5e46908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of State Space ->  24\n",
      "Size of Action Space ->  4\n",
      "Max Value of Action ->  1.0\n",
      "Min Value of Action ->  -1.0\n"
     ]
    }
   ],
   "source": [
    "num_states = saving_env.single_observation_space.shape[0]\n",
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "num_actions = saving_env.single_action_space.shape[0]\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "upper_bound = saving_env.single_action_space.high[0]\n",
    "lower_bound = saving_env.single_action_space.low[0]\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17d993cc-48d5-4d64-91ad-6e27a0885eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d93bb55-4f02-4df1-ad83-0c695d6990be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, max_size: int, observation_space: gym.spaces.Space, action_space: gym.spaces.Space, seed: int | None = 42):\n",
    "        \"\"\"Stores the replay history with a maximum of `max_size` entries, removing old entries as needed.\n",
    "\n",
    "        Parameters:\n",
    "            max_size: maximal number of entries to keep\n",
    "            observation_space: specification of the observation space\n",
    "            action_space: specification of the action space\n",
    "            seed: seed to initialize the internal random number generator for reproducibility\"\"\"\n",
    "\n",
    "        self.current_observations = np.zeros((max_size, *observation_space.shape), dtype=observation_space.dtype)\n",
    "        self.next_observations = np.zeros((max_size, *observation_space.shape), dtype=observation_space.dtype)\n",
    "        self.actions = np.zeros((max_size, *action_space.shape), dtype=action_space.dtype)\n",
    "        self.rewards = np.zeros((max_size,), dtype=np.float32)\n",
    "        self.dones = np.zeros((max_size,), dtype=np.float32)\n",
    "        \n",
    "        self.max_size = max_size\n",
    "        self.rng = np.random.default_rng(seed=seed)\n",
    "        self.buffer_pointer = 0\n",
    "        self.current_size = 0\n",
    "        \n",
    "    def add(self, current_observations: np.ndarray, actions: np.ndarray, rewards: np.ndarray, next_observations: np.ndarray, dones: np.ndarray) -> None:\n",
    "        \"\"\"Add a new entry to the buffer.\n",
    "\n",
    "        Parameters:\n",
    "            current_observations: environment state observed at the current step\n",
    "            actions: action taken by the model\n",
    "            rewards: reward received after taking the action\n",
    "            next_observations: environment state obversed after taking the action\n",
    "            dones: whether the episode has ended or not\"\"\"\n",
    "\n",
    "        batch_size = current_observations.shape[0]\n",
    "        idxs = (np.arange(batch_size) + self.buffer_pointer) % self.max_size\n",
    "\n",
    "        self.current_observations[idxs] = current_observations\n",
    "        self.actions[idxs] = actions\n",
    "        self.rewards[idxs] = rewards\n",
    "        self.next_observations[idxs] = next_observations\n",
    "        self.dones[idxs] = dones\n",
    "\n",
    "        self.buffer_pointer = (idxs[-1] + 1) % self.max_size\n",
    "        self.current_size = min(self.max_size, self.current_size + batch_size)\n",
    "    \n",
    "    def sample(self, n_samples: int, replace: bool = True) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Randomly samples `n_samples` from the buffer.\n",
    "\n",
    "        Parameters:\n",
    "            n_samples: number of samples to select\n",
    "            replace: sample with or without replacement\n",
    "\n",
    "        Returns:\n",
    "            current observations, actions, rewards, next observations, dones\"\"\"\n",
    "\n",
    "        return self[self.rng.choice(self.current_size, size=n_samples, replace=replace)]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clears the buffer\"\"\"\n",
    "\n",
    "        self.buffer_pointer = 0\n",
    "        self.current_size = 0\n",
    "\n",
    "    def __getitem__(self, index: int | np.ndarray) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Gets a sample at `index`\n",
    "\n",
    "        Parameters:\n",
    "            index: index of the sample to get\n",
    "\n",
    "        Returns:\n",
    "            current observations, actions, rewards, next observations, dones\"\"\"\n",
    "\n",
    "        return (\n",
    "            self.current_observations[index],\n",
    "            self.actions[index],\n",
    "            self.rewards[index],\n",
    "            self.next_observations[index],\n",
    "            self.dones[index]\n",
    "        )\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the number of entries in the buffer\"\"\"\n",
    "\n",
    "        return self.current_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2894c37-e95b-4c65-a668-b3d374b557b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(sampled_actions: np.ndarray):\n",
    "    sampled_actions = sampled_actions + rng.normal(loc=0, scale=0.1, size=num_actions)\n",
    "\n",
    "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "    return legal_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7efbe848-33fc-49a4-bf42-1c90620b7670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(\n",
    "    model: keras.Model,\n",
    "    buffer: ReplayBuffer,\n",
    "    env: gym.Env,\n",
    "    steps: int,\n",
    "    observations: np.ndarray | None = None,\n",
    "    one_episode: bool = False,\n",
    "    no_noise: bool = False\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Plays the environment `env` using model `model` for a total of `steps` steps.\n",
    "\n",
    "    Arguments:\n",
    "        model: model to use\n",
    "        buffer: buffer to store results to\n",
    "        env: environment to use\n",
    "        steps: total number of steps to record\n",
    "        observations: observation to start from\n",
    "        one_episode: exist as soon as one of the environments finishes\n",
    "\n",
    "    Returns:\n",
    "        the last observations\n",
    "    \"\"\"\n",
    "\n",
    "    if observations is None:\n",
    "        observations, _ = env.reset()\n",
    "\n",
    "    for _ in range(steps // env.num_envs):\n",
    "        res = model(observations, training=False).numpy()\n",
    "        \n",
    "        actions = policy(res)\n",
    "\n",
    "        if no_noise:\n",
    "            actions = res\n",
    "        \n",
    "        new_observations, rewards, terminateds, truncated, _ = env.step(actions)\n",
    "\n",
    "        equal_observations = np.all(np.isclose(observations, new_observations), axis=1)\n",
    "\n",
    "        dones = terminateds | truncated | equal_observations\n",
    "\n",
    "        rewards = np.where(equal_observations, -100, rewards)\n",
    "\n",
    "        rewards = np.where(rewards > 0, rewards * 10, rewards)\n",
    "        rewards = np.where(rewards == -100, -10, rewards)\n",
    "\n",
    "        print(rewards)\n",
    "        \n",
    "        buffer.add(\n",
    "            current_observations=observations,\n",
    "            actions=actions,\n",
    "            rewards=rewards,\n",
    "            next_observations=new_observations,\n",
    "            dones=dones,\n",
    "        )\n",
    "\n",
    "        if one_episode and np.any(dones):\n",
    "            observations = None\n",
    "            break\n",
    "        \n",
    "        observations = new_observations\n",
    "    \n",
    "    return observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "ef05c1cc-fb82-4b06-8fcd-21d1a7f84c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_model = keras.models.load_model('./history/actor-model-1.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "ee7428df-eb2a-430c-ae84-1fe7f9c73baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "99488307-d32b-459e-93f5-cc486cb68548",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 2 ** 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "ddd04360-ff01-42ec-ae03-5613f317e5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_env = gym.make_vec(\"BipedalWalker-v3\", hardcore=False, render_mode=\"rgb_array_list\", num_envs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "7752a07d-d74c-493e-9f89-2805759ee18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_gameplay(\n",
    "    model: tf.keras.Model,\n",
    "    max_steps: int = 1000,\n",
    "    env: gym.Env | None = None,\n",
    "):\n",
    "    save_buffer = ReplayBuffer(\n",
    "        max_size=buffer_size,\n",
    "        observation_space=saving_env.single_observation_space,\n",
    "        action_space=saving_env.single_action_space\n",
    "    )\n",
    "    \n",
    "    _ = play_game(\n",
    "        model=model,\n",
    "        env=env,\n",
    "        steps=max_steps,\n",
    "        buffer=save_buffer,\n",
    "        one_episode=True\n",
    "    )\n",
    "\n",
    "    render = env.render()\n",
    "\n",
    "    gym.utils.save_video.save_video(\n",
    "        frames=render[0],\n",
    "        video_folder=\"videos\",\n",
    "        fps=env.metadata[\"render_fps\"],\n",
    "    )\n",
    "\n",
    "    return render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "d6cc33a1-3dfb-4d71-a4b9-e8bcf585769c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.09208786]\n",
      "[-0.11331375]\n",
      "[-0.08701743]\n",
      "[-0.1234955]\n",
      "[-0.12667894]\n",
      "[-0.157849]\n",
      "[-0.18387067]\n",
      "[-0.17691883]\n",
      "[-0.15387657]\n",
      "[-0.06761534]\n",
      "[-0.13413068]\n",
      "[-0.10829573]\n",
      "[-0.10963248]\n",
      "[-0.0599768]\n",
      "[-0.14026337]\n",
      "[-0.02112695]\n",
      "[-0.06096963]\n",
      "[-0.11560618]\n",
      "[-0.11573885]\n",
      "[-0.09395814]\n",
      "[-0.09234012]\n",
      "[-0.08495184]\n",
      "[-0.04341763]\n",
      "[-0.0152856]\n",
      "[0.11072858]\n",
      "[-0.05313124]\n",
      "[-0.07599597]\n",
      "[-0.27257106]\n",
      "[-0.29365996]\n",
      "[-0.2688934]\n",
      "[-0.28641526]\n",
      "[-0.25984666]\n",
      "[-0.25690999]\n",
      "[-0.27868585]\n",
      "[-0.26818356]\n",
      "[-0.26776319]\n",
      "[-0.30631423]\n",
      "[-0.20908409]\n",
      "[-0.22569175]\n",
      "[-0.25414716]\n",
      "[-0.3712611]\n",
      "[-10.]\n"
     ]
    }
   ],
   "source": [
    "res = save_gameplay(actor_model, max_steps=saving_steps, env=saving_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ea7236-7802-4c5e-af2f-94a566686d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
